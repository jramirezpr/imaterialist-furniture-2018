{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f32237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import models\n",
    "import utils\n",
    "from utils import RunningMean, use_gpu\n",
    "from misc import FurnitureDataset, preprocess, preprocess_with_augmentation, NB_CLASSES, normalize_05\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 299\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    print('[+] loading model... ', end='', flush=True)\n",
    "    model = models.inceptionv4_finetune(NB_CLASSES)\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "    print('done')\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699a2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] dataset `train` loaded 116087 images from 194828\n",
      "[+] dataset `val` loaded 5112 images from 6400\n",
      "[+] loading model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "[+] nb learnable params 196736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juan.ramirezp\\Documents\\imaterialist-furniture-2018\\utils.py:37: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs = Variable(inputs, volatile=True)\n",
      " 22%|██▏       | 70/320 [06:35<22:49,  5.48s/it]"
     ]
    }
   ],
   "source": [
    "train_dataset = FurnitureDataset('train', transform=preprocess_with_augmentation(normalize_05, IMAGE_SIZE))\n",
    "val_dataset = FurnitureDataset('val', transform=preprocess(normalize_05, IMAGE_SIZE))\n",
    "training_data_loader = DataLoader(dataset=train_dataset, num_workers=8,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      shuffle=True)\n",
    "validation_data_loader = DataLoader(dataset=val_dataset, num_workers=1,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=False)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "nb_learnable_params = sum(p.numel() for p in model.fresh_params())\n",
    "print(f'[+] nb learnable params {nb_learnable_params}')\n",
    "lx, px = utils.predict(model, validation_data_loader)\n",
    "print(lx,px)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91dfc6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.890456676483154"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(Variable(px), Variable(lx)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1449b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7256 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "[+] set lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7256 [00:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8d2f17a78d64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{running_loss.value:.5f} {running_score.value:.3f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[+] epoch {epoch} {running_loss.value:.5f} {running_score.value:.3f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\imaterialist-furniture-2018\\utils.py\u001b[0m in \u001b[0;36mvalue\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrue_divide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead."
     ]
    }
   ],
   "source": [
    "min_loss = criterion(Variable(px), Variable(lx)).item()\n",
    "lr = 0\n",
    "patience = 0\n",
    "for epoch in range(20):\n",
    "    print(f'epoch {epoch}')\n",
    "    if epoch == 1:\n",
    "        lr = 0.00003\n",
    "        print(f'[+] set lr={lr}')\n",
    "    if patience == 2:\n",
    "        patience = 0\n",
    "        model.load_state_dict(torch.load('inception4_052382.pth'))\n",
    "        lr = lr / 10\n",
    "        print(f'[+] set lr={lr}')\n",
    "    if epoch == 0:\n",
    "        lr = 0.001\n",
    "        print(f'[+] set lr={lr}')\n",
    "        optimizer = torch.optim.Adam(model.fresh_params(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "\n",
    "    running_loss = RunningMean()\n",
    "    running_score = RunningMean()\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(training_data_loader, total=len(training_data_loader))\n",
    "    for inputs, labels in pbar:\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, dim=1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss.update(loss.data.item(), 1)\n",
    "        running_score.update(torch.sum(preds != labels.data), batch_size)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f'{running_loss.value:.5f} {running_score.value:.3f}')\n",
    "    print(f'[+] epoch {epoch} {running_loss.value:.5f} {running_score.value:.3f}')\n",
    "\n",
    "    lx, px = utils.predict(model, validation_data_loader)\n",
    "    log_loss = criterion(Variable(px), Variable(lx))\n",
    "    log_loss = log_loss.data.item()\n",
    "    _, preds = torch.max(px, dim=1)\n",
    "    accuracy = torch.mean((preds != lx).float())\n",
    "    print(f'[+] val {log_loss:.5f} {accuracy:.3f}')\n",
    "\n",
    "    if log_loss < min_loss:\n",
    "        torch.save(model.state_dict(), 'inception4_052382.pth')\n",
    "        print(f'[+] val score improved from {min_loss:.5f} to {log_loss:.5f}. Saved!')\n",
    "        min_loss = log_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588de85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
